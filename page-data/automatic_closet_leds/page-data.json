{"componentChunkName":"component---src-templates-post-js","path":"/automatic_closet_leds","webpackCompilationHash":"7818a87de4c3961499ae","result":{"data":{"markdownRemark":{"html":"<p><img src=\"/closet_light-1c4b740de4e537448c634035d083e9eb.gif\" alt=\"closet_light.gif\"></p>","frontmatter":{"path":"/automatic_closet_leds","title":"Automatic Closet LEDs","date":"2009-12-20","tags":["electronics","house"]}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"prev":{"html":"<p> <a href=\"/kmeans-a4ea2ef2a3b1e73f367b9147105d1faa.png\"><img src=\"/kmeans-a4ea2ef2a3b1e73f367b9147105d1faa.png\"></a></p>\n<p>I've been having a hard time <a href=\"http://www.punyblog.com/2009/12/self-organizing-maps.html\">trying</a> to train a <a href=\"http://en.wikipedia.org/wiki/Self-organizing_map\">Self-Organizing Map</a> to categorize a large pool of short documents by my selected keywords. The initial results were promising but I couldn't adjust the training parameters well enough to train the error sufficiently low that I would be confident of the categorizations, even after several days of training.</p>\n<p>Some nice folks on <a href=\"http://groups.google.com/group/comp.ai.neural-nets/topics\">comp.ai.neural-nets</a> suggested a few other techniques and I've implemented simple routines to perform <a href=\"http://en.wikipedia.org/wiki/K-means_clustering\">K-Means Clustering</a>. The categorization of my 3300 documents by 323 keywords now takes less than 10 seconds.</p>\n<p><a href=\"/k-means-270b8e72e231ca982ed3ecf7883a6b62.zip\">download my k-means c source</a><br>\n<em>(This source uses raw float arrays and includes a function to categorize the vectors in a <a href=\"http://leenissen.dk/fann/\">FANN</a> training data struct.)</em> </p>","id":"11430f21-7f93-50e4-a788-1bd1c7fc1565","frontmatter":{"path":"/k_means_clustering_code","title":"K-Means Clustering Code","date":"2009-12-24","tags":["ai","artwork","mathematics","programming"],"hexagonImages":["kmeans.png_hexagon.png"]}},"next":{"html":"<p>Lauren gave me a classic 10\" radial arm saw for my birthday after we moved (and have a garage). I've been testing its ability to cut accurately. Ripping this walnut and maple to one inch felt reasonably dangerous and I'll probably build the jig suggested in the original manual for ripping small stock.</p>\n<p><img src=\"/glueing_as-956d7a2ed8f417baeb9b5d5aed5a0f2c.jpg\" alt=\"glueing_as\"></p>\n<p>This row encodes the <a href=\"http://en.wikipedia.org/wiki/Ascii\">ASCII</a> character \"e\". \"a\" is sitting on the table.</p>\n<p><img src=\"/glueing_squares-f4e0b4ec0315e9363bfad8876704d76f.jpg\" alt=\"glueing_squares\"></p>\n<p>I decided the next cutting boards will be end-grain, so the rest of the letters for this one should be glued from single squares. This produces a less-accurate row but got the job done.</p>\n<p><img src=\"/rough_rows-996a85081b20323b174f07fb50653315.jpg\" alt=\"rough_rows\"></p>\n<p>Here are the rows of the word I planned to spell.</p>\n<p><img src=\"/final_glueing-00ce1a730c8f27efe96179d674edbb1f.jpg\" alt=\"final_glueing\"></p>\n<p>I let the last glue setup cure for 72 hours.</p>\n<p><img src=\"/oiled-2dfa3d1c56e3cc440ef83d9ef82531f3.jpg\" alt=\"oiled\"></p>\n<p>Sanded flat, edged on the router table, wetted to raise the grain and sanded to a smooth finish. Then, mineral oil. Extra points if you can tell me what word is encoded.</p>","id":"6a50cfe3-d762-5ea8-8779-d955ee290833","frontmatter":{"path":"/ascii_cutting_board","title":"ASCII Cutting Board","date":"2009-10-26","tags":["artwork","ascii","cna","house","tiles","wood"],"hexagonImages":["glueing_as.jpg_hexagon.jpeg","glueing_squares.jpg_hexagon.jpeg","rough_rows.jpg_hexagon.jpeg","final_glueing.jpg_hexagon.jpeg","oiled.jpg_hexagon.jpeg"]}}}}}